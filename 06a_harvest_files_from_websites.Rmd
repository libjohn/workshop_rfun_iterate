---
title: "Iterate crawling and downloading files"
author: "John Little"
date: "`r Sys.Date()`"
output: html_notebook
---

This lesson will show how to use the `rvest` package to facilitate web crawling for data. Below you will see an abridged presentation based on a longer [workshop on web crawling](https://rfun.library.duke.edu/portfolio/scraping_workshop/).  The downloaded data  are excel workbook files, each containing approximately 50 worksheets.  Although some likely next steps could include iterating over worksheets within each workbook, I will stop short of presenting further [data wrangling techniques for concatenating readxl workflows](https://readxl.tidyverse.org/articles/readxl-workflows.html). Additionally, because the files are voluminous, at the end of this script I include a function to delete the downloaded workbook files.  

That advanced worksheet wrangling can be gleaned by reading an excellent article on _readxl workflows_, particularly but not limited to the section _concatenate worksheets into one data frame_. 

```{r}
library(tidyverse)
library(readxl)
library(fs)
library(rvest)
```

## Data

The data we will scrape is from the [US Census pulse survey](https://www.census.gov/newsroom/press-kits/2020/pulse-surveys.html).  Sepcifically we will look at the data from the [pulse surveys household experience](https://www.census.gov/data/experimental-data-products/household-pulse-survey.html) 

The target household data is outlined at this [summary navigation page](https://www.census.gov/programs-surveys/household-pulse-survey/data.html).  [The weekly pulse surveys](https://www.census.gov/data/tables/2020/demo/hhp/hhp13.html) are linked here and each file has a different and somewhat unpredictable URL.  

> We can use `rvest` to crawl the summary page and harvest the urls of target files.  curl_download() to download each file into a target directory within the RStudio project on  the local file system. 

Here are some example files of the housing pulse survey file 2b  
- [housing2b_se_week7.xlsx](https://www2.census.gov/programs-surveys/demo/tables/hhp/2020/wk7/housing2b_se_week7.xlsx)
- [housing2b_se_week13.xlsx](https://www2.census.gov/programs-surveys/demo/tables/hhp/2020/wk13/housing2b_se_week13.xlsx)
- [housing2b_week37.xlsx](https://www2.census.gov/programs-surveys/demo/tables/hhp/2021/wk37/housing2b_week37.xlsx)
- [housing2b_week45.xlsx](https://www2.census.gov/programs-surveys/demo/tables/hhp/2022/wk45/housing2b_week45.xlsx)

`
## Set-up

Assign some object names to useful URLs and use `read_html()` to ingest the raw HTML of the example page.

```{r}
my_url <- "https://www.census.gov/programs-surveys/household-pulse-survey/data.html"
base_url <- "https://www.census.gov/"

my_results <- read_html(my_url)
```

Crawl the target webpages to gather a list of URLs for files that may be download

```{r}
link_text_1 <- my_results %>% 
  html_nodes(".uscb-title-3") %>% 
  html_text2()

link_url <- my_results %>% 
  html_nodes("a.uscb-list-item") %>% 
  html_attr("href")
  

my_crawl <- tibble(link_text_1, link_url, base_url) %>% 
  filter(str_detect(link_text_1, "Household Pulse Survey:")) %>% 
  unite(full_url, base_url, link_url, remove = TRUE, sep = "") %>% 
  mutate(full_url = str_replace(full_url, 'gov//data', 'gov/data')) %>% 
  relocate(full_url)
my_crawl
```

Use `map()` with `nest()`, `Sys.sleep()` and `libary(rvest)` to gather the target URLs that will be downloaded.

```{r}
crawl_results <- 
  my_crawl %>% 
  slice(1:6) %>% 
  select(link_text_1, full_url) %>% 
  nest(data = -link_text_1) %>% 
  mutate(my_rawhtml = map(data, ~ {
    Sys.sleep(2)    ## DO THIS.  Pause 2 seconds between each file.  ##
    .x %>%
      pull(full_url) %>% 
      read_html() %>% 
      html_nodes("a") %>%
      html_attr("href") %>% 
      tibble() 
    }))
```


Subset the list of downloadable files to only the `housing2b` files.

```{r}
download_target_urls <- crawl_results %>% 
  unnest(my_rawhtml) %>% 
  rename(rawhtml = 3) %>% 
  filter(str_detect(rawhtml, "housing2b")) %>% 
  mutate(download_xworkbook_url = str_glue("https:{rawhtml}") ) %>% 
  mutate(my_filename = fs::path_file(rawhtml)) 
download_target_urls
```

Download the files 

```{r}
fs::dir_create("data/xl_workbooks")

walk2(download_target_urls$download_xworkbook_url, 
      str_glue("data/xl_workbooks/{download_target_urls$my_filename}"), 
      curl::curl_download, mode = "wb")
```


## Put a bow on it.

Below shows loading a single excel file from the several that were downloaded.  Read more about [readxl workflows](https://readxl.tidyverse.org/articles/readxl-workflows.html)

```{r}
my_files <- fs::dir_ls("data/xl_workbooks", glob = "*.xlsx")

my_files <- enframe(my_files) %>% 
  filter(str_detect(name, "2b_w")) %>% 
  slice(1) %>%
  pull(name)
my_files

my_xl_df <- read_xlsx(my_files,
                      sheet = "NC", 
                      skip = 4)

my_xl_df <- my_xl_df %>% 
  janitor::clean_names()

my_xl_df %>% 
  filter(select_characteristics != "Total") %>% 
  mutate(sub_group = if_else(is.na(total1), select_characteristics, NA_character_)) %>% 
  fill(sub_group, .direction = "down") %>% 
  relocate(sub_group) %>% 
  filter(sub_group == "Education")
```
## Delete the files downloaded

```{r}
fs::dir_delete("data/xl_workbooks")
```

