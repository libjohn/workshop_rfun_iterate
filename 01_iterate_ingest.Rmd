---
title: "Iterate Ingest"
output: html_notebook
---

```{r}
library(tidyverse)
library(fs)
```

```{r}
my_files <- fs::dir_ls("data", glob = "*.csv")
my_files 

my_big_df <- read_csv(my_files, id = "filename")
```


```{r}
my_big_df 
  # count(rideable_type) %>% 
```

## purrr - first look

using `map()` to iterate a function by row of a tibble

Example:  
`map(my_df, my_func)`

In this case, we want to force the column data type using a `read_csv()` argument: `col_types`.  We can still do this without `purrr::map()` but the code chunk below introduces you to the functions within `map()`. The purrr functions are powerful.  We will talk more about this concept.

```{r}
readr::read_csv(my_files, id = "filename", 
                col_types = cols(start_station_id = col_character()))
```


### map

```{r}
foo_df <- map_df(my_files, read_csv, 
                 col_types = cols(start_station_id = col_character(),
                                  end_station_id = col_character()),
                 id = "filename")

foo_df
```


### Excel

You can do a similar iteration with excel worksheests within excel workbooks.  A well composed [article explains these excel workflows](https://readxl.tidyverse.org/articles/readxl-workflows.html) combining the power of the `purrr` and `readxl` packages. 

## download some large files

Get IMDB data from https://www.imdb.com/interfaces/

```{r}
fs::dir_create("data")

if(!fs::file_exists("data/title.ratings.tsv.gz")) {
  download.file("https://datasets.imdbws.com/title.ratings.tsv.gz", "data/title.ratings.tsv.gz")  
}

if(!fs::file_exists("data/name.basics.tsv.gz")) {
  download.file("https://datasets.imdbws.com/name.basics.tsv.gz", "data/name.basics.tsv.gz")  
}
```


## clean names

The `janitor` packages has a handy function:  `clean_names()`.  This helps keep your variable names tidy.

```{r}
imdb_df <- read_tsv("data/title.ratings.tsv.gz", n_max = 1000)
imdb_df

imdb_df %>% 
  janitor::clean_names()

imdb_names_df <- read_tsv("data/name.basics.tsv.gz", n_max = 1000) %>% 
  janitor::clean_names()
imdb_names_df
```

## Multi-value fields

Introducing `separate_rows` and `separate()` plus a very early view at using a regular expression, or regex to find patterns in text.  IN this case, counting the commas in a multi-value field.

```{r}
imdb_names_df %>% 
  separate_rows(primary_profession) %>% 
  count(primary_name, sort = TRUE)

imdb_names_df %>% 
  mutate(prolficacy = str_count(known_for_titles, ",") + 1) %>% 
  separate(known_for_titles, into = c("foo_1", "foo_2", "foo_3", "foo_4"), sep = ",")
```


The process of using separate_rows has the effect of making the tibble **tall** rather than wide.  Making data tall or wide  is typically accomplished with the `pivot_longer` and `pivot_wider` functions.  A good reason to pivot tall is to leverage the convenience of iteration.  In the example below, `separate_rows` effectively pivots a 1000 observations (rows) tibble into a nearly 3000 rows tibble.  In this way, separate_rows is a specialized compound function using `separate()`, `pivot_longer()`, and `drop_na()`.  By making these transformations, it becomes easier iterate in ggplot2, ie. to calculate the frequency of categories for a bar plot. 

```{r}
imdb_names_df %>% 
  separate_rows(primary_profession, sep = ",") %>% 
  ggplot(aes(y = fct_rev(fct_infreq(primary_profession)))) +
  geom_bar()

# imdb_names_df %>% 
#   select(primary_name, primary_profession) %>% 
#   separate(primary_profession, into = c("foo_1", "foo_2", "foo_3"), sep = ",") %>% 
#   pivot_longer(foo_1:foo_3, names_to = "prof_field", values_to = "primary_profession") %>% 
#   drop_na(primary_profession) %>% 
#   count(primary_profession)  %>% 
#   ggplot(aes(y = reorder(primary_profession, n), x = n)) +
#   geom_col()
```



